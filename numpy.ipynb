{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are working on a natural language processing (NLP) project where you have developed a text classification model to categorize customer reviews into positive and negative sentiments. The model has been trained on a dataset, and now you want to evaluate its performance.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Example: Predicted_labels (1 for positive, O for negative)\n",
    "predicted_labels = np.array([1,1,0,1,0,1,0,1,1,0])\n",
    "\n",
    "Example: True_labels (1 for positive, O for negative)\n",
    "true_labels = np.array([1,0,0,1,1,1,0,0,1,0])\n",
    "\n",
    "\n",
    "What is the accuracy of the text classification model?\n",
    "Calculate the precision of the model for positive sentiment.\n",
    "Determine the recall of the model for negative sentiment.\n",
    "Compute the F1 score of the text classification model.\n",
    "Calculate the confusion matrix for the model's predictions.\n",
    "What is the prevalence of positive sentiment in the true labels?\n",
    "Compute the specificity of the model for positive sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Example: Predicted_labels (1 for positive, O for negative)\n",
    "predicted_labels = np.array([1,1,0,1,0,1,0,1,1,0])\n",
    "\n",
    "# Example: True_labels (1 for positive, O for negative)\n",
    "true_labels = np.array([1,0,0,1,1,1,0,0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 7\n"
     ]
    }
   ],
   "source": [
    "# What is the accuracy of the text classification model?\n",
    "\n",
    "accuracy = np.sum(predicted_labels == true_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for Positive Sentiment: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Calculate the precision of the model for positive sentiment.\n",
    "true_positive = np.sum((predicted_labels == 1) & (true_labels == 1))\n",
    "false_positive = np.sum((predicted_labels == 1) & (true_labels == 0))\n",
    "\n",
    "precision_positive = true_positive / (true_positive + false_positive)\n",
    "print(\"Precision for Positive Sentiment:\", precision_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for Negative Sentiment: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Determine the recall of the model for negative sentiment.\n",
    "true_negative = np.sum((predicted_labels == 0) & (true_labels == 0))\n",
    "false_negative = np.sum((predicted_labels == 0) & (true_labels == 1))\n",
    "\n",
    "recall_negative = true_negative / (true_negative + false_negative)\n",
    "print(\"Recall for Negative Sentiment:\", recall_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7058823529411765\n"
     ]
    }
   ],
   "source": [
    "# Compute the F1 score of the text classification model.\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "recall = true_negative / (true_negative + false_negative)\n",
    "\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[3 2]\n",
      " [1 4]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the confusion matrix for the model's predictions.\n",
    "conf_matrix = np.array([[true_negative, false_positive],\n",
    "                        [false_negative, true_positive]])\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prevalence of Positive Sentiment in True Labels: 0.5\n"
     ]
    }
   ],
   "source": [
    "# What is the prevalence of positive sentiment in the true labels?\n",
    "prevalence_positive = np.mean(true_labels == 1)\n",
    "print(\"Prevalence of Positive Sentiment in True Labels:\", prevalence_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity for Positive Sentiment: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Compute the specificity of the model for positive sentiment.\n",
    "specificity_positive = true_negative / (true_negative + false_positive)\n",
    "print(\"Specificity for Positive Sentiment:\", specificity_positive)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
